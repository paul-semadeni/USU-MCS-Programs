{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Assignment 5&6\n",
    "        Class: Intro to Data Analysis (CS6850)\n",
    "        Instructor: Dr. Hamid Karimi\n",
    "        Date: March 13, 2024\n",
    "        Student: Paul Semadeni\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "import random\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Draw the boxplot in Python.\n",
    "data = np.array([-1,3,7,8,-15,7,2,1,25])\n",
    "q1 = np.percentile(data, 25)\n",
    "median = np.median(data)\n",
    "q3 = np.percentile(data, 75)\n",
    "iqr = q3 - q1\n",
    "lower_bound = q1 - 1.5 * iqr\n",
    "upper_bound = q3 + 1.5 * iqr\n",
    "outliers = data[(data < lower_bound) | (data > upper_bound)]\n",
    "print(\"\\nQ1:\", q1, \"\\nMedian:\", median, \"\\nQ3:\", q3, \"\\nIQR:\", iqr, \"\\nLower bound:\", lower_bound, \"\\nUpper bound:\", upper_bound, \"\\nOutliers:\", outliers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.boxplot(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Draw parallel coordinates for vehicles dataset. Show that parallel coordinates are sensitive to the order of attributes by drawing several plots with different orders of columns\n",
    "with open(\"./files/vehicles.csv\") as file:\n",
    "    vehicles_df = pd.read_csv(file)\n",
    "vehicles_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.parallel_coordinates(vehicles_df, \"class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_list = list(range(19))\n",
    "random.shuffle(temp_list)\n",
    "columns = [vehicles_df.columns[i] for i in temp_list]\n",
    "print(columns)\n",
    "reorder_vehicles_df = vehicles_df[columns]\n",
    "chart = pd.plotting.parallel_coordinates(reorder_vehicles_df, \"class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_list = list(range(19))\n",
    "random.shuffle(temp_list)\n",
    "columns = [columns[i] for i in temp_list]\n",
    "reorder_vehicles_df = vehicles_df[columns]\n",
    "chart = pd.plotting.parallel_coordinates(reorder_vehicles_df, \"class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_list = list(range(19))\n",
    "random.shuffle(temp_list)\n",
    "columns = [columns[i] for i in temp_list]\n",
    "reorder_vehicles_df = vehicles_df[columns]\n",
    "chart = pd.plotting.parallel_coordinates(reorder_vehicles_df, \"class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement the Misra-Gries algorithm in Python and execute it for the stream of tokensin tweet words one week.txt\n",
    "def misra_gries(k):\n",
    "    f = open(\"./files/tweet_words_one_week.txt\", \"r\")\n",
    "    buffer_dict = dict()\n",
    "    total_dict = dict()\n",
    "    m = 0\n",
    "    n = 0\n",
    "    while True:\n",
    "        token = f.readline()\n",
    "        if not token:\n",
    "            print(\"End of the stream\")\n",
    "            break\n",
    "        # Stream length\n",
    "        m += 1\n",
    "        # Add unique items to dictionary, increment non-unique entries\n",
    "        if token in buffer_dict:\n",
    "            buffer_dict[token] += 1\n",
    "        elif len(buffer_dict) < k - 1:\n",
    "            buffer_dict[token] = 1\n",
    "            # Unique item count\n",
    "            n += 1\n",
    "        else:\n",
    "            for i in buffer_dict.copy():\n",
    "                buffer_dict[i] -= 1\n",
    "                if buffer_dict[i] == 0:\n",
    "                    buffer_dict.pop(i)\n",
    "        if token not in total_dict:\n",
    "            total_dict[token] = 0\n",
    "        total_dict[token] += 1\n",
    "    # Calculate false positives\n",
    "    target_frequency = m / k\n",
    "    false_positives = 0\n",
    "    true_positives = 0\n",
    "    for i in buffer_dict:\n",
    "        i_frequency = total_dict[i]\n",
    "        if i_frequency >= target_frequency:\n",
    "            true_positives += 1\n",
    "        else:\n",
    "            false_positives += 1\n",
    "    print(\"k:\", k, \"\\nm:\", m, \"\\nn:\", n, \"\\nbuffer:\", buffer_dict, \"\\nnumber of false positives:\", false_positives, \"\\nnumber of true positives:\", true_positives)\n",
    "\n",
    "b = 50\n",
    "misra_gries(b)\n",
    "print(\"Buffer size:\", b, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Run your implementations for buffer sizes {300, 800, 1000, 5000, 10000}, and for each buffer size, calculate the proportion of the false positives.\n",
    "buffer_sizes = [300, 800, 1000, 5000, 10000]\n",
    "for b in buffer_sizes:\n",
    "    misra_gries(b)\n",
    "    print(\"Buffer size:\", b, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Extract rules using mlxtend package similar to what was presented in the lecture.Note that you need to convert the data to a pandas DataFrame. You can consider each item as an attribute and each transaction as a data point. Print out the frequent itemsets as well as rules with their support and confidence. Did you get the same results with manual execution?\n",
    "def convert_to_one_hot(col):\n",
    "    return pd.get_dummies(data[col], prefix=col)\n",
    "\n",
    "transactions = {\n",
    "1: {\"Bread\", \"Coffee\", \"Sugar\"},\n",
    "2: {\"Bread\", \"Milk\"},\n",
    "3: {\"Bread\", \"Butter\", \"Milk\"},\n",
    "4: {\"Coffee\", \"Milk\"},\n",
    "5: {\"Bread\", \"Butter\", \"Cookies\"},\n",
    "6: {\"Coffee\", \"Milk\", \"Sugar\"},\n",
    "7: {\"Bread\", \"Eggs\", \"Milk\", \"Sugar\"},\n",
    "8: {\"Bread\", \"Butter\", \"Cookies\"},\n",
    "9: {\"Bread\", \"Butter\", \"Eggs\", \"Milk\"},\n",
    "10: {\"Coffee\", \"Eggs\", \"Milk\"}\n",
    "}\n",
    "transactions_df = pd.DataFrame.from_dict(transactions, orient=\"index\")\n",
    "# Convert to one hot\n",
    "one_hot_data = []\n",
    "list_of_cols = []\n",
    "for c in transactions_df.columns:\n",
    "    d = convert_to_one_hot(c)\n",
    "    for n in (d.columns):\n",
    "        list_of_cols.append(n)\n",
    "    one_hot_data.append(d)\n",
    "one_hot_data = pd.concat(one_hot_data, ignore_index=True, axis=1)\n",
    "one_hot_data.set_axis(list_of_cols,  axis=1)\n",
    "\n",
    "freq_items = apriori(one_hot_data, min_support=0.4, use_colnames=True)\n",
    "freq_items.sort_values(\"support\", ascending=False)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
